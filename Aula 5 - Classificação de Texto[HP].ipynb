{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção: Rode esta linha apenas se estiver usando o Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "'World',\n",
    "'Sports',\n",
    "'Business',\n",
    "'Sci/Tech',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O código da célula abaixo contém funções para efetuar a carga dos dados, treinamento teste dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do dataset\n",
    "import os \n",
    "if not os.path.exists('agnews.zip'):\n",
    "    !wget https://s3-us-west-2.amazonaws.com/wehrmann/agnews.zip\n",
    "    !unzip agnews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "def load_vocab(src):\n",
    "    with open(src) as f:\n",
    "        d = json.load(f)\n",
    "    vocab = Vocabulary()\n",
    "    vocab.word2idx = d['word2idx']\n",
    "    vocab.idx2word = d['idx2word']\n",
    "    vocab.idx = d['idx']\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(txt):\n",
    "    return open(txt).read().strip().split('\\n')\n",
    "\n",
    "\n",
    "def get_xy(raw_data):\n",
    "    classes = []\n",
    "    texts = []\n",
    "    for line in raw_data:\n",
    "        y, x = line.split('\\t')\n",
    "        y = np.int(y)\n",
    "        classes.append(y)\n",
    "        texts.append(x)\n",
    "    return classes, texts\n",
    "\n",
    "\n",
    "def tokenize_text(text, vocab, to_tensor=True):\n",
    "    # Convert caption (string) to word ids.\n",
    "    tokens = nltk.tokenize.word_tokenize(\n",
    "        str(text).lower()#.decode('utf-8')\n",
    "    )\n",
    "    caption = []\n",
    "    caption.append(vocab('<start>'))\n",
    "    caption.extend([vocab(token) for token in tokens])\n",
    "    caption.append(vocab('<end>'))\n",
    "    if to_tensor:\n",
    "        caption = torch.Tensor(caption)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, data_split,):\n",
    "        self.vocab = load_vocab(os.path.join(data_path, 'vocab.json'))\n",
    "        self.raw_data = load_txt(\n",
    "            os.path.join(data_path,'{}.csv'.format(data_split))            \n",
    "        )\n",
    "        self.split = data_split\n",
    "\n",
    "        self.labels, self.texts = get_xy(self.raw_data)\n",
    "        assert len(self.labels) == len(self.texts)\n",
    "        self.nb_classes = np.max(self.labels) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        tokens = tokenize_text(data, vocab=self.vocab)\n",
    "\n",
    "        return tokens, label, index, data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # Sort a data list by caption length\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    captions, labels, ids, raw = zip(*data)\n",
    "    labels = torch.Tensor(labels).long()\n",
    "    # Merget captions (convert tuple of 1D tensor to 2D tensor)\n",
    "    captions, lengths = pad_default(captions)\n",
    "\n",
    "    return captions, labels, lengths\n",
    "\n",
    "\n",
    "def pad_default(captions):\n",
    "    lengths = np.array([len(cap) for cap in captions])\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    \n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    \n",
    "    return targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "        data_path,\n",
    "        batch_size,\n",
    "        workers=2, \n",
    "        splits=['train', 'val', 'test'], \n",
    "    ):\n",
    "\n",
    "    loaders = []\n",
    "    for split in splits:\n",
    "        csv_dataset = CSVDataset(\n",
    "            data_path=data_path,\n",
    "            data_split=split,\n",
    "        )\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset=csv_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(split == 'train'),            \n",
    "            collate_fn=collate_fn,            \n",
    "        )\n",
    "        loaders.append(loader)\n",
    "\n",
    "    return tuple(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "        model, \n",
    "        device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        epoch, \n",
    "        log_interval\n",
    "    ):\n",
    "    model.train()\n",
    "    history = []\n",
    "    for batch_idx, (data, target, lengths) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, lengths)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(\n",
    "        model, \n",
    "        device, \n",
    "        criterion, \n",
    "        test_loader\n",
    "    ):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, lengths in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data, lengths)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        device,\n",
    "        lr,\n",
    "        nb_epochs=3,\n",
    "        log_interval=100,\n",
    "    ):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for epoch in range(1, nb_epochs + 1):\n",
    "        print('\\n* * * Training * * *')\n",
    "        train_epoch(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            train_loader=train_loader, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            epoch=epoch, \n",
    "            log_interval=log_interval\n",
    "        )\n",
    "        print('\\n* * * Evaluating * * *')\n",
    "        acc = test(model, device, criterion, test_loader)        \n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def check_input(model, device):\n",
    "    dummy_data = torch.ones(5, 30).long().to(device)\n",
    "    lens = [28]*5\n",
    "    dummy_pred = model(dummy_data, lens)\n",
    "    assert dummy_pred.shape == (5, 4), '\\nOutput expected: (batch_size, 4) \\nOutput found   : {}'.format(dummy_pred.shape)\n",
    "    print('Passed')\n",
    "    return dummy_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parâmetros que você pode definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device_name = 'cpu'\n",
    "nb_epochs = 3\n",
    "log_interval = 100\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferência dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loaders('./agnews/', batch_size=batch_size, splits=['train', 'test'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train size: ', 112400, 112400)\n",
      "('Test size : ', 7600, 7600)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Train size: ', \n",
    "    len(train_loader.dataset.texts),\n",
    "    len(train_loader.dataset.labels)\n",
    ")\n",
    "print(\n",
    "    'Test size : ', \n",
    "    len(test_loader.dataset.texts),\n",
    "    len(test_loader.dataset.labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_instance(instance_id):\n",
    "    print('\\nExample: ')\n",
    "    print(train_loader.dataset.texts[instance_id])\n",
    "    print('\\nLabel Number: ')\n",
    "    print(train_loader.dataset.labels[instance_id])\n",
    "    print('\\nLabel String: ')\n",
    "    print(classes[train_loader.dataset.labels[instance_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example: \n",
      "Computer Q amp;A: Tips on installing Windows XP SP2. It seems that Microsoft #39;s new upgrade to Windows is making some people nervous. That #39;s not surprising, as home and office computing environments are just not as friendly as they used to be due to viruses and spyware. Even the software we ...\n",
      "\n",
      "Label Number: \n",
      "3\n",
      "\n",
      "Label String: \n",
      "Sci/Tech\n",
      "\n",
      "Example: \n",
      "Ace performance by Thailand #39;s Thongchai at Mount Juliet. Thailand #39;s Thongchai Jaidee aced the 165-yard 11th hole in the WGC-American Express Championship second round on Friday. The 34-year-old former paratrooper used an eight \n",
      "\n",
      "Label Number: \n",
      "1\n",
      "\n",
      "Label String: \n",
      "Sports\n",
      "\n",
      "Example: \n",
      "Office Depot Won #39;t Meet Analysts #39; Earnings Estimates for 3 Qtr. Office Depot Inc., the world #39;s No. 2 office-supplies retailer, said it expects earnings per share for the third quarter to fall below current First Call estimates partly because of the recent Hurricanes in Florida.\n",
      "\n",
      "Label Number: \n",
      "2\n",
      "\n",
      "Label String: \n",
      "Business\n",
      "\n",
      "Example: \n",
      "Powell Appeals to Arafat to Step Aside.  NEW YORK (Reuters) - Secretary of State Colin Powell  appealed to Palestinian President Yasser Arafat on Thursday to  step aside for the sake of achieving his lifetime goal of a  Palestinian state.\n",
      "\n",
      "Label Number: \n",
      "0\n",
      "\n",
      "Label String: \n",
      "World\n"
     ]
    }
   ],
   "source": [
    "plot_instance(0)\n",
    "\n",
    "plot_instance(5000)\n",
    "\n",
    "plot_instance(1238)\n",
    "\n",
    "plot_instance(8723)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Instance Example: ', torch.Size([128, 104]), torch.Size([128]))\n"
     ]
    }
   ],
   "source": [
    "text, labels, lens = next(iter(train_loader))\n",
    "print('Instance Example: ', text.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([     1,  68545,  33965,  82460,  40004,  18495,  54068,    506,\n",
      "         77001,  33965,  82460,  36958,  82460,  69217,  24511,   5727,\n",
      "          8850,  64317,  24511,  77001,  14712,  42781,  64317,  69681,\n",
      "         73290,  18507,  64317,   3769,  45875,  67568,  48864,  42781,\n",
      "         80438,  77001,  30730,  75507,  33965,  82460,  36958,  52788,\n",
      "         48864,  91161,   8795,  94949,  72329,  69319,   7904,  96393,\n",
      "         94950,  73612,  73612,  21049,  42781,  21187,   1528,  72465,\n",
      "         42781,  80706,  42781,  82930,  73612,  73612,  17824,  73612,\n",
      "         73612,  77575,  73612,  73612,    984,  58126,  73612,  73612,\n",
      "         87101,  18424,  73612,  73612,  72329,  17227,   7904,  72329,\n",
      "         69319,   7904,  68669,  72329,  17227,   7904,  56004,  30163,\n",
      "         76572,  72329,  69319,   7904,  78778,  72329,  17227,   7904,\n",
      "         72329,  69319,   7904,   5536,  72329,  17227,   7904,      2]), tensor(3), 104)\n"
     ]
    }
   ],
   "source": [
    "print(text[0], labels[0], lens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(train_loader.dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_vector(texts, lengths):    \n",
    "    I = torch.LongTensor(lengths).view(-1, 1, 1)\n",
    "    I = I.expand(texts.size(0), 1, texts[0].size(1))-1\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        I = I.cuda()\n",
    "    \n",
    "    out = torch.gather(texts, 1, I).squeeze(1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seu trabalho começa aqui:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crie uma rede neural, usando `nn.LSTM()` ou `nn.GRU()` para classificar os textos.\n",
    "\n",
    "\n",
    "* Utilize a rede DigitsConvNet para processar cada um dos frames. \n",
    "* Utilize uma rede recorrente da sua escolha para processar a dimensão temporal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Stuff\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=97585, embedding_dim=100)\n",
    "        self.rnn = nn.LSTM(input_size=100, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 4)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        x, _ = self.rnn(packed)\n",
    "        padded = pad_packed_sequence(x, batch_first=True)\n",
    "        hidden, _ = padded\n",
    "        vector = get_txt_vector(hidden, lengths)\n",
    "        x = self.fc(vector)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextLSTM(\n",
      "  (embedding): Embedding(97585, 100)\n",
      "  (rnn): LSTM(100, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TextLSTM().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "dummy_pred = check_input(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/112400 (0%)]\tLoss: 1.391728\n",
      "Train Epoch: 1 [64000/112400 (57%)]\tLoss: 0.537255\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0037, Accuracy: 6372/7600 (83.84%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/112400 (0%)]\tLoss: 0.245913\n",
      "Train Epoch: 2 [64000/112400 (57%)]\tLoss: 0.319304\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0028, Accuracy: 6722/7600 (88.45%)\n",
      "\n",
      "Final acc: 88.45%\n"
     ]
    }
   ],
   "source": [
    "acc = train(model, train_loader, test_loader, device, lr, nb_epochs=2, log_interval=log_interval)\n",
    "print('Final acc: {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crie uma rede neural, a mais rápida o possível, para classificar textos.\n",
    "\n",
    "LSTMs e GRUs são lentas, procure usar uma abordagem mais rápida (ex: conv1d, average pooling, self-attention, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FastNet, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred = check_input(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = train(model, train_loader, test_loader, device, lr, nb_epochs, log_interval)\n",
    "print('Final acc: {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implemente uma rede neural, da sua escolha, para classificar textos. Seu objetivo é conseguir a acurácia mais alta da turma.\n",
    "\n",
    "Você pode escolher todos os elementos da arquitetura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourBestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourBestNet, self).__init__()                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Verifique se a saída do seu modelo está correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = YourBestNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred = check_input(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Treine seu modelo por algumas épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = train(model, train_loader, test_loader, device, lr, nb_epochs, log_interval)\n",
    "print('Final acc: {:.2f}%'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vse27",
   "language": "python",
   "name": "vse27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
